# Q-Learning
[link](https://thomassimonini.medium.com/)
## What is RL? A short recap

In RL, we build an agent that can **make smart decisions**. For instance, an agent that **learns to play a video game.** Or a trading agent that **learns to maximize its benefits** by making smart decisions on **what stocks to buy and when to sell.**



![1614816098068](./files/Q_Learning/1614816098068.png)	

But, to make smart decisions, our agent will learn from the environment by **interacting with it through trial and error** and receiving rewards (positive or negative) **as unique feedback.**

Its goal **is to maximize its expected cumulative reward** (because of the reward hypothesis).

**The agentâ€™s brain is called the policy Ï€.** Itâ€™s where the agent makes its decision-making process: given a state, our policy will output an action or a probability distribution over actions.

![1614816154852](./files/Q_Learning/1614816154852.png)

**Our goal is to find an optimal policy Ï€\***, aka, a policy that leads to the best expected cumulative reward.

And to find this optimal policy (hence solving the RL problem) there **are two main types of RL methods**:

- *Policy-based-methods*: **Train our policy directly** to learn which action to take, given a state.
- *Value-based methods*: **Train a value function** to learn **which state is more valuable** and using this value function **to take the action that leads to it.**

![1614816183867](./files/Q_Learning/1614816183867.png)

æ¥ä¸‹æ¥ä¸»è¦è®²çš„æ˜¯ Value-based methods



## The two types of value-based methods

In value-based methods, **we learn a value function,** that **maps a state to the expected value of being at that state.**

åœ¨ åŸºäºä»·å€¼çš„æ–¹æ³•ä¸­ï¼Œ æˆ‘ä»¬å­¦ä¹ äº†ä¸€ä¸ª ä»·å€¼å‡½æ•°ï¼Œå°†çŠ¶æ€æ˜ å°„åˆ°å¤„äºè¯¥çŠ¶æ€çš„æœŸæœ›å€¼ã€‚

![1614816323304](./files/Q_Learning/1614816323304.png)



The value of a state is the **expected discounted return** the agent can get if it **starts at that state, and then acts according to our policy.**



But what means acting according to our policy? We donâ€™t have a policy in value-based methods since we train a value-function and not a policy?







Remember that the goal of an **RL agent is to have an optimal policy Ï€\*.**



To find it, we learned that there are two different methods:



* *Policy-based methods*ï¼š **Directly train the policy** to select what action to take given a state (or a probability distribution over actions at that state). In this case, we **donâ€™t have a value-function.**

![1614816584832](./files/Q_Learning/1614816584832.png)



And consequently, **we donâ€™t define by hand the behavior of our policy, itâ€™s the training that will define it.**

ç›´æ¥è®­ç»ƒè¿™ç§ç­–ç•¥å»é€‰æ‹© åœ¨æŸç§çŠ¶æ€ä¸‹æ‰§è¡Œå“ªç§è¡Œä¸ºï¼Œè¿™ç§è¶‹å‘äºå›ºå®šçš„é€‰æ‹©







- *Value-based methods:* **Indirectly, by training a value-function** that outputs the value of a state, or a state-action pair. Given this value function, our policy **will take action.**



But, because we didnâ€™t train our policy, **we need to specify its behavior.**

ä½†æ˜¯è¿™ç§åŸºäºä»·å€¼çš„æ–¹æ³•ï¼Œæ²¡æœ‰è®­ç»ƒæˆ‘ä»¬çš„ç­–ç•¥ï¼Œæ‰€ä»¥æˆ‘ä»¬è¦è§„å®šä»–çš„è¡Œä¸º



For instance, if we want a policy that given the value function will take actions that always lead to the biggest value, **weâ€™ll create a Greedy Policy.**

ä¾‹å¦‚ï¼Œæˆ‘ä»¬æ€»æ˜¯æƒ³è¦æœ‰æœ€å¤§å€¼çš„é‚£ç§åŠ¨ä½œï¼Œ æ‰€ä»¥è¿™æ—¶å€™å°±ä¼šæœ‰ä¸€ä¸ª è´ªå©ªç­–ç•¥ Greedy Policy



![1614816713532](./files/Q_Learning/1614816713532.png)

Consequently, whatever method you use to solve your problem, **you will have a policy**, but in the case of value-based methods you donâ€™t train it, your policy **is just a simple function that you specify** (for instance greedy policy) and this policy **uses the values given by the value-function to select its actions.**

æ€»è€Œè¨€ä¹‹ï¼Œæ— è®ºä»€ä¹ˆæ–¹æ³•ï¼Œéƒ½ä¼šæœ‰ä¸€ä¸ªç­–ç•¥ï¼Œ ä½†æ˜¯ åŸºäºä»·å€¼çš„æ–¹æ³• ï¼ˆvalue-basedï¼‰ æ²¡æœ‰è¿›è¡Œè®­ç»ƒï¼Œ ç­–ç•¥å°±åªæ˜¯ä¸€ä¸ªç®€å•çš„å‡½æ•°ã€‚åŸºäºè¿™ä¸ªå‡½æ•°çš„å€¼è¿›è¡Œé€‰æ‹©



So the difference is, in policy-based **the optimal policy is found by training the policy directly.**

In value-based, **finding an optimal value-function leads to having an optimal policy.**

![1614817268246](./files/Q_Learning/1614817268246.png)

In fact, in value-based methods, most of the time youâ€™ll use **an Epsilon-Greedy Policy** that handles the exploration/exploitation trade-off



So, we have two types of value-based functions:

### The State-Value function

We write the state value function under a policy Ï€ like this:![1614817352279](./files/Q_Learning/1614817352279.png)

è¿™ä¸ªå…¬å¼è¡¨æ˜ï¼Œ åœ¨è®¡ç®—value çš„æ—¶å€™ï¼Œç”¨çš„éƒ½æ˜¯ç›¸åŒçš„  Ï€ ç­–ç•¥ï¼Œä¾‹å¦‚ï¼Œé€‰æ‹©å€¼æœ€å¤§çš„é‚£ä¸ªåŠ¨ä½œï¼Œè¿™å°±æ˜¯ä¸€ä¸ªç­–ç•¥ï¼Œå¦‚æœä¸€ä¸ªè¿‡ç¨‹ä¸­æœ‰ä¸¤ä¸ªç­–ç•¥ï¼Œå°±å¯ä»¥ç”¨ä¸¤ä¸ª  Ï€ è¡¨ç¤ºäº†ï¼Œå°±è¯´æ˜ï¼Œä¸€ä¸ªè¿‡ç¨‹ä¸­é‡‡ç”¨äº†ä¸¤ç§æ–¹æ³•å–å€¼ã€‚

åœ¨æ¯ä¸€æ­¥ï¼ŒåŸºäºåŒæ ·çš„é€‰æ‹©ç­–ç•¥é€‰æ‹©è¡Œä¸ºã€‚å› ä¸ºç­–ç•¥å¯ä»¥æœ‰å¤šç§ï¼Œè¿™æ ·å°±å¯ä»¥çœ‹åˆ°ä¸€ä¸ªè¿‡ç¨‹ä¸­åˆ°åº•æœ‰å¤šå°‘ä¸ªç­–ç•¥äº†	



For each state, the state-value function outputs the expected return if the agent **starts at that state,** and then follow the policy forever after (for all future timesteps if you prefer).

æœ‰ä¸€ä¸ªstateï¼Œ å°±èƒ½å¾—åˆ°ä¸€ä¸ªå€¼value



![1614817411918](./files/Q_Learning/1614817411918.png)



### The Action-Value function

In the Action-value function, for each state and action pair, the action-value function **outputs the expected return,** if the agent starts in that state and takes the action, and then follows the policy forever after.

The value of taking action A in state S under a policy Ï€ is:

![1614817621678](./files/Q_Learning/1614817621678.png)



å°±æ˜¯åœ¨ä¸€ä¸ªçŠ¶æ€ stateä¸‹ï¼Œ æ‰§è¡ŒæŸä¸ªåŠ¨ä½œåï¼Œå†è®¡ç®—å‡ºä¸€ä¸ªå€¼



![1614817627083](./files/Q_Learning/1614817627083.png)







We see that the difference is:

- In state-value function, we calculate **the value of a state (St).** åœ¨è¿™ä¸ªå‡½æ•°ä¸‹ï¼Œæˆ‘ä»¬è¿”å›çš„æ˜¯åŸºäºstate è®¡ç®—çš„å€¼
- In action-value function, we calculate **the value of state-action pair (St, At) hence the value of taking that action at that state.** åœ¨è¿™ä¸ªå‡½æ•°ä¸‹ï¼Œæˆ‘ä»¬è¿”å›çš„æ˜¯åŸºäº state å’Œ action çš„å€¼

![1614817707437](./files/Q_Learning/1614817707437.png)

Whatever value function we choose (state-value or action-value function), **the value is the expected return.**

æ— è®ºæˆ‘ä»¬é€‰æ‹©çš„æ˜¯é‚£ç§å‡½æ•°ï¼Œå€¼éƒ½æ˜¯æœŸæœ›è¿”å›çš„ ï¼ˆè¿™é‡Œå¯èƒ½æ˜¯æœŸæœ›å€¼ï¼Œå°±æ˜¯å¹³å‡å€¼ï¼‰



However, the problem is that it implies that **to calculate EACH value of a state or a state-action pair, we need to sum all the rewards an agent can get if it starts at that state.**

ä½†æ˜¯ï¼Œé—®é¢˜åœ¨äºï¼Œæ— è®ºæ˜¯ ä¸€ä¸ªçŠ¶æ€å€¼ï¼Œè¿˜æ˜¯çŠ¶æ€-åŠ¨ä½œå¯¹çš„å€¼ï¼Œæˆ‘ä»¬éƒ½å¾—æŠŠè¿™äº›å€¼å…¨éƒ¨ç›¸åŠ æ‰è¡Œ

This can be a dull process and thatâ€™s **where the Bellman equation comes to help us.**

è¿™æ˜¯ä¸€ä¸ªå¾ˆç¹çæ¯ç‡¥çš„è¿‡ç¨‹ï¼Œæ­¤æ—¶ï¼ŒBellman equationå¯ä»¥å¸®æˆ‘ä»¬è§£å†³è¿™ä¸€é—®é¢˜



## The Bellman Equation: simplify our value estimation

The Bellman equation **simplifies our state value or state-action value calculation.**

è¿™ä¸ªç­‰å¼ ç®€åŒ–äº† çŠ¶æ€å€¼æˆ–è€…æ˜¯çŠ¶æ€-åŠ¨ä½œå€¼çš„è®¡ç®—



With what we learned from now, we know that if we calculate the V(St) (value of a state), we need to calculate the return starting at that state then follow the policy forever after. **(Our policy that we defined in the next example is a Greedy Policy and for simplification, we donâ€™t discount the reward).**

æ ¹æ®æˆ‘ä»¬ç°åœ¨å­¦åˆ°çš„ï¼Œæˆ‘ä»¬çŸ¥é“å¦‚æœæˆ‘ä»¬è®¡ç®—V(St)(çŠ¶æ€çš„å€¼)ï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—ä»é‚£ä¸ªçŠ¶æ€å¼€å§‹çš„æ”¶ç›Šï¼Œç„¶åæ°¸è¿œéµå¾ªç­–ç•¥



ä¸‹è¾¹çš„ä¾‹å­éƒ½æ˜¯ Greedy Policyï¼Œ ä¸ºäº†ç®€åŒ–ï¼Œæ²¡æœ‰æŠ˜æ‰£ç‡



ä¾‹ï¼š

So to calculate V(St) we need to make the sum of the expected rewards.

ä¸ºäº†è®¡ç®— V(St), æˆ‘ä»¬å°†æ¯ä¸ªæœŸæœ›å€¼ç›¸åŠ ï¼Œ expected reward. æˆ‘ä»¬å¸Œæœ›ï¼Œ è¿™ä¸ªæœŸæœ›çš„å€¼æ˜¯ -1ï¼Œæ‰€ä»¥æ˜¯å°†æ‰€æœ‰çš„-1ç›¸åŠ 

![1614818250725](./files/Q_Learning/1614818250725.png)







Then, to calculate the V(St+1), we need to calculate the return starting at that state St+1. 

ç„¶åå»è®¡ç®— V(St + 1) çš„å€¼ï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—ä»çŠ¶æ€St+1å¼€å§‹çš„è¿”å›å€¼

![1614818436985](./files/Q_Learning/1614818436985.png)

So you see thatâ€™s a quite dull process if you need to do it for each state value or state-action value.



Instead of calculating for each state or each state-action pair the expected return**, we can use the Bellman equation.**



### The Bellman equation 

The Bellman equation is a recursive equation that works like thisï¼š

**instead of starting for each state from the beginning** and calculating the return, we can consider the value of any state as:

Bellmanæ–¹ç¨‹æ˜¯ä¸€ä¸ª**é€’å½’æ–¹ç¨‹**ï¼Œå…¶å·¥ä½œåŸç†æ˜¯è¿™æ ·çš„:æˆ‘ä»¬å¯ä»¥è€ƒè™‘ä»»æ„çŠ¶æ€çš„å€¼ï¼Œè€Œä¸æ˜¯ä»å¤´å¼€å§‹è®¡ç®—æ¯ä¸ªçŠ¶æ€çš„è¿”å›å€¼:



**The immediate reward (Rt+1) + the discounted value of the state that follows (gamma \* V(St+1)).**

å³æ—¶å¥–åŠ±(Rt+1) +éšåçŠ¶æ€çš„æŠ˜ç°å€¼(gamma * V(St+1))ã€‚



![1614818541771](./files/Q_Learning/1614818541771.png)

If we go back to our example, the value of State 1= expected cumulative return if we start at that state.

![1614818614626](./files/Q_Learning/1614818614626.png)

Which is equivalent to V(St) = Immediate reward (Rt+1) + Discounted value of the next state (Gamma * V(St+1)).

![1614819641422](./files/Q_Learning/1614819641422.png)

- The value of V(St+1) = Immediate reward (Rt+2) + Discounted value of the St+2 (Gamma * V(St+2)).
- And so on.

å°±æ˜¯è¯´ï¼Œè¿™ä¸ª Bellman equation æ˜¯ä¸€ä¸ªé€’å½’æ–¹ç¨‹ï¼Œå¯ä»¥ä»ä»»æ„çš„ä¸€ä¸ªçŠ¶æ€å¼€å§‹ï¼Œé€šè¿‡é©¬ä¸Šè¦è·å–çš„ä»·å€¼ï¼ŒåŠ ä¸Šä¸‹ä¸€ä¸ªçŠ¶æ€çš„ä»·å€¼ï¼Œå°±å¯ä»¥è·å–å½“å‰çš„æ€»ä»·å€¼äº†ã€‚ 

ä¹‹å‰æ˜¯æŠŠæ¯ä¸ªå€¼éƒ½åŠ èµ·æ¥ï¼Œä»–ä»¬çš„æ€»å’Œï¼Œæ„æˆäº†ä¸€ä¸ªä¸ªçš„çŠ¶æ€å€¼ã€‚ calculating each value as the sum of the expected return.



To recap, the idea of the Bellman equation is that instead of calculating each value as the sum of the expected return, **which is a long process.** This is equivalent **to the sum of immediate reward + the discounted value of the state that follows.**



## Monte Carlo vs Temporal Difference Learning

 the two ways of learning whatever the RL method we use.

å­¦ä¹ RLæ–¹æ³•çš„ä¸¤ç§å­¦ä¹ æ–¹æ³•

Remember that an RL agent **learns by interacting with its environment.** The idea is that **using the experience taken**, given the reward he gets, it will **update its value or its policy.**



Monte Carlo and Temporal Difference Learning are two different **strategies on how to train our value function or our policy function.** Both of them **use experience to solve the RL problem.**

Monte Carlo and Temporal Difference Learning æ˜¯ä¸¤ç§ä¸åŒçš„ç­–ç•¥æ¥è®­ç»ƒæˆ‘ä»¬çš„ä»·å€¼å‡½æ•°æˆ–æ”¿ç­–å‡½æ•°ã€‚ä»–ä»¬éƒ½ç”¨**ç»éªŒ**æ¥è§£å†³RLé—®é¢˜ã€‚





But on the one hand, Monte Carlo uses **an entire episode of experience before learning.** On the other hand, Temporal Difference uses **only a step (St, At, Rt+1, St+1) to learn.**

Weâ€™ll explain both of them **using a value-based method example.**



### Monte Carlo: learning at the end of the episode

Monte Carlo waits until the end of the episode, then calculates Gt (return) and uses it as **a target for updating V(St).**

Monte Carlo ç­‰åˆ°åˆ°episode çš„ç»ˆç‚¹ï¼Œç„¶åè®¡ç®— Gtï¼Œ å¹¶ä¸”ä½¿ç”¨Gtå»æ›´æ–° V(St)



So it requires a **complete entire episode of interaction before updating our value function.**

![1614820467956](./files/Q_Learning/1614820467956.png)

If we take an example:

![1614820486152](./files/Q_Learning/1614820486152.png)

- We always start the episode **at the same starting point.**
- **We try actions using our policy** (for instance using Epsilon Greedy Strategy, a policy that alternates between exploration (random actions) and exploitation. will be introduced later).

- We get **the Reward and the Next State.**
- We terminate the episode if the cat eats us or if we move > 10 steps.
- At the end of the episode, **we have a list of State, Actions, Rewards, and Next States.**
  - æœ€åï¼Œæˆ‘ä»¬æœ‰äº†ä¸€ä¸ª state, action, rewards, and next states çš„åˆ—è¡¨
- **The agent will sum the total rewards Gt** (to see how well it did).
- It will then **update V(st) based on the formula.**![1614820738808](./files/Q_Learning/1614820738808.png)

* Then **start a new game with this new knowledge**

ä»¥ä¸Šï¼Œå°±æ˜¯ Mote Carlo è¿‡ç¨‹ï¼Œ è¿ç”¨çš„æ˜¯æ•´ä¸ªè¿‡ç¨‹ä¹‹åå¾—å‡ºçš„å€¼ï¼Œè¿™ä¸ªè¿‡ç¨‹çš„ç»ˆæ­¢ç”±è‡ªå·±è®¾ç½®ï¼Œå¯èƒ½ æ˜¯æ­¥æ•° >10 åœæ­¢ï¼Œæˆ–è€…æ˜¯æ‰¾åˆ°ç›®æ ‡ååœæ­¢ï¼Œç„¶åè¿›è¡Œè®¡ç®—ï¼Œå¾—å‡ºè¿”å›å€¼Gtï¼Œä»¥æ­¤ä¸ºç»éªŒè¿›è¡Œæ›´æ–°ã€‚

By running more and more episodes, **the agent will learn to play better and better.**



å¦‚æœæˆ‘ä»¬è¿ç”¨è¿™ä¸ªè¿‡ç¨‹è®­ç»ƒä¸€ä¸ªå‡½æ•°ï¼š

For instance, if we train a state-value function using Monte Carlo:

- We just started to train our Value function **so it returns 0 value for each state.**

- Our learning rate (lr) is 0.1 and our discount rate is 1 (= no discount).
- Our mouse, **explore the environment and take random actions**, we see what it did here:
- ![1614821048980](./files/Q_Learning/1614821048980.png)

- The mouse made more than 10 steps, so the episode ends.
- We have a list of state, action, rewards, next_state, **we need to calculate the return Gt.**
- Gt = Rt+1 + Rt+2 + Rt+3â€¦ (for simplicity we donâ€™t discount the rewards).

- Gt = 1 + 0 + 0 + 0+ 0 + 0 + 1 + 1+ 0 + 0
  - æœ‰å¥¶é…ªåŠ 1ï¼Œ æ²¡æœ‰å¥¶é…ª 0
- Gt= 3
- We can now update V(S0):
- New V(S0) = V(S0) + lr * [Gt â€” V(S0)]
- New V(S0) = 0 + 0.1 * [3 â€“0]
- The new V(S0) = 0.3



### Temporal Difference Learning: learning at each step

- **Temporal Difference, on the other hand, waits for only one interaction (one step) St+1** to form a TD target and update V(St) using Rt+1 and gamma * V(St+1).

æ—¶é—´å·®å¼‚ä»…ç­‰å¾…ä¸€ä¸ªäº¤äº’ï¼ˆä¸€æ­¥ï¼‰St + 1å½¢æˆTDç›®æ ‡ï¼Œå¹¶ä½¿ç”¨Rt + 1å’Œgamma * Vï¼ˆSt + 1ï¼‰æ›´æ–°Vï¼ˆStï¼‰ã€‚



The idea is that with **TD we update the V(St) at each step.**

æ¯ä¸€æ­¥éƒ½æ›´æ–°



But because we didnâ€™t play during an entire episode, we donâ€™t have Gt (expected return), **instead, we estimate Gt by adding Rt+1 and the discounted value of next state.**

å› ä¸ºæ²¡æœ‰Gt, æ‰€ä»¥æˆ‘ä»¬é€šè¿‡æ·»åŠ äº† Rt + 1 å’Œ æŠ˜æ‰£å€¼ æ¥ä¼°è®¡ Gt





We speak about **bootstrap because TD bases its update part on an existing estimate V(St+1) and not a full sample Gt.**

æˆ‘ä»¬è°ˆè®ºå¼•å¯¼ç¨‹åºæ˜¯å› ä¸ºTDå°†å…¶æ›´æ–°éƒ¨åˆ†åŸºäºç°æœ‰ä¼°è®¡Vï¼ˆSt + 1ï¼‰è€Œä¸æ˜¯å®Œæ•´çš„æ ·æœ¬Gtã€‚



![1614821706371](./files/Q_Learning/1614821706371.png)

This method is called TD(0) or **one step TD (update the value function after any individual step).**



If we take the same example,

![1614821741556](./files/Q_Learning/1614821741556.png)

- We just started to train our Value function so it returns 0 value for each state.
  - åˆšå¼€å§‹çš„å€¼éƒ½æ˜¯0
- Our learning rate (lr) is 0.1 and our discount rate is 1 (no discount).
- Our mouse, explore the environment and take a random action: **the action going to the left.**
- It gets a reward Rt+1 = 1 since **it eat a piece of cheese.**

![1614821864787](./files/Q_Learning/1614821864787.png)

We can now update V(S0):

New V(S0) = V(S0) + lr * [R1 + gamma * V(S1) â€” V(S0)]

New V(S0) = 0 + 0.1 * [1 + 0.99 * 0â€“0]

The new V(S0) = 0.1

So we just updated our value function for State 0.

Now we **continue to interact with this environment with our updated value function.**



... ...



![1614822101388](./files/Q_Learning/1614822101388.png)



# Introducing Q-Learning



Q-Learning is an **off-policy value-based method that uses a TD approach to train its action-value function:**

æ‰€ä»¥ï¼Œè¦å­¦ä¹ Q-learningï¼Œé¦–å…ˆéœ€è¦çŸ¥é“å‡ ä¸ªåè¯ï¼Œ **off-policy, value-based method. TD approach** å’Œ **action-value**



Q-Learning is value-based method ï¼Œ ä½¿ç”¨ TD approach æ¥ è®­ç»ƒ action-value å‡½æ•°

- *â€œOff-policyâ€*: weâ€™ll talk about that at the end of this chapter.
- *â€œValue-based methodâ€*: it means that it finds its optimal policy indirectly by training a value-function or action-value function that will tell us whatâ€™s **the value of each state or each state-action pair.**
- *â€œUses a TD approachâ€*: **updates its action-value function at each step.**



In fact, **Q-Learning is the algorithm we use to train our Q-Function**, an **action-value function** that determines the value of being at a certain state, and taking a certain action at that state.

Q-learning æ˜¯æˆ‘ä»¬ç”¨æ¥è®­ç»ƒQ å‡½æ•°çš„ä¸€ç§ç®—æ³•ï¼Œ ä¸€ç§è¡Œä¸ºä»·å€¼å‡½æ•°ï¼Œå®ƒå†³å®šåœ¨æŸä¸€çŠ¶æ€ä¸‹çš„ä»·å€¼ï¼Œå¹¶åœ¨è¯¥çŠ¶æ€ä¸‹é‡‡å–æŸä¸€è¡Œä¸ºã€‚



å°±æ˜¯ä¸€ç›´è®­ç»ƒå¹¶æ›´æ–°Qè¡¨ï¼Œè®­ç»ƒå®Œæˆä¹‹åï¼Œä¼šæœ‰ä¸€ä¸ªæœ€ç»ˆçš„Qè¡¨ï¼Œç„¶åï¼Œç”¨è¿™ä¸ªQè¡¨è¿›è¡Œæ£€æµ‹ï¼Œçœ‹çœ‹è®­ç»ƒçš„è¿‡ç¨‹æ˜¯ä¸æ˜¯æœ‰æ•ˆï¼Œæˆ–è€…è¯´æ˜¯æ•ˆæœæ¯”è¾ƒå¥½ã€‚

æœŸä¸­ï¼Œå¦‚ä½•æ›´æ–°Qè¡¨ï¼Œå°±æ˜¯å†³ç­–æ€§çš„é—®é¢˜ï¼ŒåŒ…å«ç€å¯èƒ½ä¸åªä¸€ç§ç­–ç•¥ ï¼Œä¸€èˆ¬å°±ç”¨Ï€è¡¨ç¤ºäº†ï¼Œä¸€ä¸ªÏ€å°±è¡¨ç¤ºä¸€ä¸ªç­–ç•¥ï¼Œå½“ç„¶æ˜¯ä¸åŒçš„ï¼Œå¿…é¡» Ï€1ï¼ŒÏ€â€˜ ä»€ä¹ˆçš„ï¼Œæœ€ç»ˆç”Ÿæˆçš„Qè¡¨ï¼Œé‡Œè¾¹åŒ…å«äº†å„ç§çŠ¶æ€ä¸‹ï¼Œå„ç§åŠ¨ä½œå¯ä»¥å¾—åˆ°çš„ reward. æ‰€ä»¥ï¼Œæœ€ç»ˆçš„è¿™ä¸ªQè¡¨ï¼Œå°±æ˜¯æœ€ç»ˆçš„ç­–ç•¥ policy Ï€











![1614822356546](./files/Q_Learning/1614822356546.png)

The **Q comes from â€œthe Qualityâ€ of that action at that state.**

Internally, our Q-function has **a Q-table, which is a table where each cell corresponds to a state-action value pair value.** Think of this Q-table as **the memory or cheat sheet of our Q-function.**

If we take this maze example:

![1614822485497](./files/Q_Learning/1614822485497.png)



The Q-Table (just initialized thatâ€™s why all values are = 0), **contains for each state, the 4 state-action values.**

Q-Table è¿›è¡Œåˆå§‹åŒ–ï¼Œ å…¨éƒ½æ˜¯0ï¼Œ åŒ…æ‹¬æ¯ä¸€ç§çŠ¶æ€-è¡Œä¸ºå€¼

![1614822549509](./files/Q_Learning/1614822549509.png)

Here we see that the **state-action value of the initial state and going up is 0:**

è¿™é‡Œï¼Œç”»è“çº¿çš„åœ°æ–¹ï¼Œå‘ä¸Šèµ°çš„å€¼æ˜¯0



![1614822564407](./files/Q_Learning/1614822564407.png)



Therefore, Q-Function contains a Q-table **that contains the value of each-state action pair.** åœ¨è€é¼ å½“å‰çš„çŠ¶æ€ä¸‹ï¼Œå‘ä¸Šçš„åŠ¨ä½œï¼Œå€¼ä¸º0

 given a state and action, **our Q-Function will search inside its Q-table to output the value.**

![1614822693562](./files/Q_Learning/1614822693562.png)



So, if we recap:

- The *Q-Learning* **is the RL algorithm that**
- Trains *Q-Function*, an **action-value function** that contains, as internal memory, a *Q-table* **that contains all the state-action pair values.**
- Given a state and action, our Q-Function **will search into its Q-table the corresponding value.**
- When the training is done, **we have an optimal Q-Function, so an optimal Q-Table.**
- And if we **have an optimal Q-function**, we **have an optimal policy,** since we **know for each state, what is the best action to take.**



![1614822791023](./files/Q_Learning/1614822791023.png)

But, in the beginning, **our Q-Table is useless since it gives arbitrary value for each state-action pair** (most of the time we initialize the Q-Table to 0 values)ï¼ŒBut, as weâ€™ll **explore the environment and update our Q-Table it will give us better and better approximations.**

æœ€åˆçš„æ—¶å€™ï¼ŒQ-table æ²¡æœ‰ç”¨ï¼Œå› ä¸ºé‡Œè¾¹çš„å€¼åˆšå¼€å§‹éƒ½æ˜¯ä»»æ„çš„ï¼Œä¸è¿‡ä¸€èˆ¬ä¼šåˆå§‹åŒ–ä¸º0ï¼Œ ä½†æ˜¯éšç€æˆ‘ä»¬çš„ä¸æ–­çš„æ¢ç´¢æ›´æ–°Q-table, è¿™ä¸ªQ-table å°±ä¼šè¶Šæ¥è¶Šå¥½äº†

![1614822931104](./files/Q_Learning/1614822931104.png)



We see here that with the training, our Q-Table is better since thanks to it we can know the value of each state-action pair.





## The Q-Learning algorithm



This is the Q-Learning pseudocode, letâ€™s study each part, **then weâ€™ll see how it works with a simple example before implementing it.**

![1614822986748](./files/Q_Learning/1614822986748.png)



Example:

**Step 1: We initialize the Q-Table**

![1614823015245](./files/Q_Learning/1614823015245.png)

We need to initialize the Q-Table for each state-action pair. **Most of the time we initialize with values of 0.**

åˆå§‹åŒ–



**Step 2: Choose action using Epsilon Greedy Strategy**

![1614823038876](./files/Q_Learning/1614823038876.png)



Epsilon Greedy Strategy is a policy that handles the exploration/exploitation trade-off.



The idea is that we define epsilon É› = 1.0:

- *With probability 1 â€” É›* : we do **exploitation** (aka our agent selects the action with the highest state-action pair value).
- With probability É›: **we do exploration** (trying random action).



At the beginning of the training, **the probability of doing exploration will be very big since É› is very high, so most of the time weâ€™ll explore.** But as the training goes, and consequently our **Q-Table gets better and better in its estimations, we progressively reduce the epsilon value** since we will need less and less exploration and more exploitation.



è¿™æ˜¯è¯´ï¼Œåˆšå¼€å§‹åˆå§‹åŒ– É› = 1.0ï¼Œ æœ‰1-É› = 1-1 = 0 çš„æ¦‚ç‡ è¿›è¡Œ exploitationï¼Œ æœ‰  É› = 1.0  çš„æ¦‚ç‡è¿›è¡Œ exploration, æ‰€ä»¥åˆšå¼€å§‹ï¼Œä¼šè¿›è¡Œ exploration, ç„¶åéšç€æ•°æ®çš„æ›´æ–°ï¼Œ É› ä¼šé€æ¸å˜å°ï¼Œ ç›¸åº”çš„ï¼Œè¿›è¡Œ exploitation çš„æ¦‚ç‡ä¹Ÿä¼šä¸€ç‚¹ç‚¹å˜å¤§

![1614823278827](./files/Q_Learning/1614823278827.png)



**Step 3: Perform action At, gets Rt+1 and St+1**

æ‰§è¡Œ At è¡Œä¸ºï¼Œ è·å– ä¸‹ä¸€ä¸ª Reward  å€¼ å’Œ çŠ¶æ€å€¼ St + 1

![1614823316443](./files/Q_Learning/1614823316443.png)



**Step 4: Update Q(St, At)**



Remember that in TD Learning, we update our policy or value function (depending on the RL method we choose) **after one step of interaction.**

To produce our TD target, **we used the immediate reward Rt+1 plus the discounted value of the next state best state-action pair** (we call that bootstrap).

![1614823498593](./files/Q_Learning/1614823498593.png)

Therefore, our Q(St, At) **update formula goes like this:**

![1614823510164](./files/Q_Learning/1614823510164.png)

It means that to update our Q(St,At):



- We need St, At, Rt+1, St+1.
- To update our Q-value at this state-action pair, we form our TD target:



We use Rt+1 and to get the **best next-state-action pair value,** we select with a greedy-policy **(so not our epsilon greedy policy)** the next best action (so the action that have the highest state-action value).

Then when the update of this Q-value is done. We start in a new_state and select our action **using our epsilon-greedy policy again.**



**Itâ€™s why we say that this is an off-policy algorithm.**



## Off-policy vs On-policy

The difference is subtle:



### Off-policy

- *Off-policy*: using **a different policy for acting and updating.**



â€‹	For instance, with Q-Learning, the Epsilon greedy policy (acting policy), is different from the greedy policy that is **used to select the best next-state action value to update our Q-value (updating policy).**



![1614823902272](./files/Q_Learning/1614823902272.png)



å°±æ˜¯è¯´ï¼Œ  Epsilon greedy policy è¿™æ˜¯ä¸€ä¸ªåŠ¨ä½œç­–ç•¥ï¼Œæ˜¯ç”¨äºä¸‹ä¸€æ­¥è¯¥æ€ä¹ˆèµ°çš„ç­–ç•¥ï¼Œç„¶åï¼Œè¿˜æœ‰ä¸€ä¸ª greedy policyï¼Œ æ˜¯ç”¨äºé€‰æ‹©ä¸‹ä¸€ä¸ªçŠ¶æ€æ—¶å€™æ‰§è¡Œç›¸åº”åŠ¨ä½œåï¼Œå¾—åˆ°çš„æœ€å¤§å€¼çš„ç­–ç•¥ï¼Œä»è€Œæ›´æ–°Q-table å‘ä¸‹è¿›è¡Œã€‚



Is different from the policy we use during the training part:

![1614824075951](./files/Q_Learning/1614824075951.png)



### On-policy

- *On-policy:* using the **same policy for acting and updating.**

For instance, with Sarsa, another value-based algorithm, **itâ€™s the Epsilon-Greedy Policy that selects the next_state-action pair, not a greedy-policy.**

![1614824279800](./files/Q_Learning/1614824279800.png)

![1614824287294](./files/Q_Learning/1614824287294.png)



### An example

![1614824310184](./files/Q_Learning/1614824310184.png)

- Youâ€™re a mouse in this very small maze. You always **start at the same starting point.**

- The goal is **to eat the big pile of cheese at the bottom right-hand corner,** and avoid the poison.
- The episode ends if we eat the poison, **eat the big pile of cheese or if we spent more than 5 steps.**
- The learning rate is 0.1
- The gamma (discount rate) is 0.99

The reward function goes like this:

- **+0:** Going to a state with no cheese in it.
- **+1:** Going to a state with a small cheese in it.
- **+10:** Going to the state with the big pile of cheese.
- **-10:** Going to the state with the poison and thus die

To train our agent to have an optimal policy (so a policy that goes left, left, down). **We will use the Q-Learning algorithm.**





**Step 1: We initialize the Q-Table**

![1614824372287](./files/Q_Learning/1614824372287.png)

So, for now, **our Q-Table is useless**, we need **to train our Q-Function using Q-Learning algorithm.**

Letâ€™s do it for 2 steps:

**Step 2: Choose action using Epsilon Greedy Strategy**

Because epsilon is big = 1.0, I take a random action, in this case I go right.

![1614824436293](./files/Q_Learning/1614824436293.png)

**Step 3: Perform action At, gets Rt+1 and St+1**

By going right, Iâ€™ve got a small cheese so Rt+1 = 1 and Iâ€™m in a new state.

![1614824469573](./files/Q_Learning/1614824469573.png)

**Step 4: Update Q(St, At)**

We can now update Q(St, At) using our formula.![1614824483473](./files/Q_Learning/1614824483473.png)

STEP 2:

**Step 2: Choose action using Epsilon Greedy Strategy**

**I take again a random action, since epsilon is really big 0.99** (since we decay it a little bit because as the training progress we want less and less exploration).



I took action down. **Not a good action since it leads me to the poison.**

**Step 3: Perform action At, gets Rt+1 and St+1**

Because I go to the poison state, **I get Rt+1 = -10 and I die.**

**Step 4: Update Q(St, At)**

![1614824573380](./files/Q_Learning/1614824573380.png)

Because weâ€™re dead, we start a new episode. But what we see here, is that **with two explorations steps, my agent became smarter.**

As we continue to explore and exploit the environment and update Q-values using TD target, **Q-Table will give us better and better approximations. And thus, at that end of the training, weâ€™ll get an optimal Q-Function.**



å…¶å®ï¼Œ åœ¨è®¾è®¡ç¯å¢ƒå®éªŒçš„è¿‡ç¨‹ä¸­ï¼Œ å¥–åŠ±å’Œæƒ©ç½šä»¥åŠç»“æŸçš„æ¡ä»¶ï¼Œéƒ½éœ€è¦è¿›è¡Œæ¨¡æ‹Ÿçš„èµ‹å€¼ï¼Œè§„å®šå¥½è¿™äº›ç‰¹æ®Šåœ°ç‚¹çš„æ•°å€¼ï¼Œæ‰€ä»¥è¿™äº›å‚æ•°å¯ä»¥çœ‹æˆæ˜¯å·²çŸ¥çš„ï¼Œæˆ‘ä»¬è¦åšçš„å°±æ˜¯å°†æœ€åçš„ Q-table ä¸­çš„æ•°å€¼ï¼Œåœ¨ç›¸åº”çš„ç­–ç•¥ä¸‹ï¼Œé€šè¿‡ä¸åŒçš„çŠ¶æ€å’ŒåŠ¨ä½œï¼Œå°½å¯èƒ½çš„å°†æœ€ç»ˆçš„ ç´¯è®¡å¥–åŠ±ï¼ˆcumulative rewardï¼‰æœ€å¤§åŒ–ï¼Œå°±å¯ä»¥äº†ã€‚



# Letâ€™s train our Q-Learning Taxi agent ğŸš•



Now that we understood the theory behind Q-Learning, **letâ€™s implement our first agent.**



![1614824702376](./files/Q_Learning/1614824702376.png)

The goal here is to train a taxi agent **to navigate in this city to transport its passengers from point A to point B.**

![1614824716319](./files/Q_Learning/1614824716319.png)



Our environment looks like this, i**tâ€™s a 5x5 grid world,** our taxi is spawned randomly in a square. The passenger is **spawned randomly** in one of the 4 possible locations (R, B, G, Y) and **wishes to go in one of the 4 possibles locations too.**

![1614824896062](./files/Q_Learning/1614824896062.png)

Your task is to **pick up the passenger at one location and drop him off in its desired location** (selected randomly).

**There are 6 possible actions,** the actions are deterministic (it means the one you choose to take is the one you take):

![1614824920167](./files/Q_Learning/1614824920167.png)

The reward system:
![1614824929557](./files/Q_Learning/1614824929557.png)

Remember that the goal of our agent is to maximize its expected cumulative reward, **if the reward is -1, its goal is to have the minimum amount possible of negative reward** (since he wants to maximize the sum), so it will **push him to go the faster possible.** So to take the passenger from his location to its destination as fast as possible.

